{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func,FeatureExtractor, ImputeNA, CategoricalEncoding,text\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score, make_scorer,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import make_pipeline, make_union \n",
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "import bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interaction(feature_list):\n",
    "    total = len(feature_list)*(len(feature_list)-1)/2\n",
    "    step = 0\n",
    "    for i,ai in enumerate(feature_list):\n",
    "        for j,bj in enumerate(feature_list):\n",
    "            if i<j:\n",
    "                x = total_data[ai]\n",
    "                y = total_data[bj]\n",
    "                t = []\n",
    "                for l in range(total_data.shape[0]):\n",
    "                    t.append(str(x[l])+' '+ str(y[l]))\n",
    "                total_data[ai+'_'+bj] = t\n",
    "                step +=1\n",
    "                bar.drawProgressBar(step/total)\n",
    "                \n",
    "                \n",
    "def upper_prob(data):\n",
    "    uppercase = []\n",
    "    total = len(data)\n",
    "    step = 0\n",
    "    for i in data:\n",
    "        length = len(i.split())\n",
    "        tmp = []\n",
    "        for j in i:\n",
    "            if j.isupper():\n",
    "                tmp.append(j)\n",
    "        uppercase.append(len(tmp)/length)\n",
    "    return(uppercase)\n",
    "\n",
    "def scale(data):\n",
    "    data = np.array(data).reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return(data)\n",
    "\n",
    "def tokenize_stop(text):\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "\n",
    "def stop_and_max_feature(data, top_frequent_num, word_least_frequency_num,scale_v,stop_v):\n",
    "    x_lower = [sublist.lower() for sublist in data]\n",
    "    x_lower = [tokenize_stop(i) for i in x_lower]\n",
    "    x_unlist = []\n",
    "    for i in x_lower:\n",
    "        x_unlist += i\n",
    "    vocab_dic = Counter(x_unlist)\n",
    "    stopwords_num = top_frequent_num\n",
    "\n",
    "    print('      total vocab: ',len(vocab_dic.most_common()))\n",
    "    maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>word_least_frequency_num])\n",
    "    print('      vocab size frequency >', word_least_frequency_num, ': ', maxfeature)\n",
    "\n",
    "    stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "\n",
    "    x_n_level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "    x_n_level_unlist = [[] for i in range(len(np.unique(y_total)))]\n",
    "    for i in range(len(x_n_level)):\n",
    "        for j in x_n_level[i]:\n",
    "            x_n_level_unlist[i] += j\n",
    "        \n",
    "    multilevel_vocab = []\n",
    "    for i in range(len(np.unique(y_total))):\n",
    "        multilevel_vocab.append(Counter(x_n_level_unlist[i]))\n",
    "\n",
    "    multilevel_stop = defaultdict(list)\n",
    "    for i in range(len(np.unique(y_total))):\n",
    "        tt = len(x_n_level_unlist[i])\n",
    "        for j in stop:\n",
    "            multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "    stop_var = [(key,np.std(value)*scale_v) for key,value in multilevel_stop.items() ]\n",
    "    stop = [i[0] for i in stop_var if i[1]<stop_v]\n",
    "    return(stop,maxfeature)\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('#','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('#1','')\n",
    "        text = text.replace('#2','')\n",
    "        text = text.replace('#3','')\n",
    "        text = text.replace('#4','')\n",
    "        text = text.replace('#5','')\n",
    "        text = text.replace('#6','')\n",
    "        text = text.replace('#7','')\n",
    "        text = text.replace('#8','')\n",
    "        text = text.replace('#9','')\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Data......\n",
      "Generate Interaction Features Between Categorical Features......\n",
      "[ ==================== ] 100.00%\n",
      "time elapsed:  50.74075436592102 \n",
      "\n",
      "Calculate Uppercase Probability to Features......\n",
      "time elapsed:  1.1400959491729736 \n",
      "\n",
      "Choose Specific Stop Words and Max Text Feature Number......\n",
      "      total vocab:  49793\n",
      "      vocab size frequency > 0 :  49793\n",
      "time elapsed:  1.766697645187378 \n",
      "\n",
      "-------Transform to Features(word tf-idf level)-------\n",
      "time elapsed:  73.19932651519775\n",
      "      Add Length and Upper Prob to Features......\n",
      "      X_word_tfidf shape:  (178778, 50239)\n",
      "      time elapsed:  30.5923912525177\n",
      "-------Word TF-IDF Data Complete-------\n",
      "\n",
      "-------Ridge Model-------\n",
      "      train accuracy: 2.66386078617\n",
      "      time elapsed:  34.059736490249634\n",
      "-------Word TF-IDF Ridge Model Complete-------\n",
      "\n",
      "-------Transform to Features(char level)-------\n",
      "time elapsed:  58.46917462348938\n",
      "      Add Length and Upper Prob to Features......\n",
      "      X_char shape:  (178778, 22258)\n",
      "      time elapsed:  31.602362632751465\n",
      "-------Character Data Complete-------\n",
      "\n",
      "-------Ridge Model-------\n",
      "      train accuracy: 3.20560649173\n",
      "      time elapsed:  80.2731237411499\n",
      "-------Character Ridge Model Complete-------\n",
      "\n",
      "-------Transform to Features(count level)-------\n",
      "time elapsed:  161.50917410850525\n",
      "      Add Length and Upper Prob to Features......\n",
      "      X_word_cv shape:  (178778, 50239)\n",
      "      time elapsed:  36.30888390541077\n",
      "-------Word Count Vectorize Data Complete-------\n",
      "\n",
      "-------Ridge Model-------\n",
      "      train accuracy: 2.33681763094\n",
      "      time elapsed:  90.43989109992981\n",
      "-------Word Count Vectorize Ridge Model Complete-------\n",
      "\n",
      "-------Ensemble 3 Results-------\n"
     ]
    }
   ],
   "source": [
    "print('Read Data......')\n",
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "test = pd.read_csv('test_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "total_data = pd.concat([train,test],ignore_index=True)\n",
    "\n",
    "print('Generate Interaction Features Between Categorical Features......')\n",
    "s = time.time()\n",
    "cate_list = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "generate_interaction(cate_list)            \n",
    "print('\\ntime elapsed: ', time.time()-s,'\\n')\n",
    "\n",
    "x_total = list(total_data.comment)\n",
    "y_total = list(train.score)\n",
    "\n",
    "print('Calculate Uppercase Probability to Features......')\n",
    "s = time.time()\n",
    "upper_p = upper_prob(x_total)\n",
    "new_up = scale(upper_p)\n",
    "print('time elapsed: ', time.time()-s,'\\n')\n",
    "\n",
    "print('Choose Specific Stop Words and Max Text Feature Number......')\n",
    "s=time.time() \n",
    "stop, maxfeature = stop_and_max_feature(x_total,250,0,1000,0.1)\n",
    "print('time elapsed: ', time.time()-s,'\\n')\n",
    "\n",
    "# Cate pipeline\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "\n",
    "print('-------Transform to Features(word tf-idf level)-------')\n",
    "s=time.time()\n",
    "comment_word_tfidf_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union_word_tfidf = make_union(\n",
    "    onehot_pipeline,\n",
    "    comment_word_tfidf_pipeline\n",
    ")\n",
    "X_word_tfidf = feature_union_word_tfidf.fit_transform(total_data)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('      Add Length and Upper Prob to Features......')\n",
    "s = time.time()\n",
    "length = [(X_word_tfidf[i,]!=0).sum() for i in range(X_word_tfidf.shape[0])]\n",
    "new_l = scale(length)\n",
    "X_word_tfidf = hstack([X_word_tfidf,new_l],format='csr')\n",
    "X_word_tfidf = hstack([X_word_tfidf,new_up],format='csr')\n",
    "\n",
    "print('      X_word_tfidf shape: ',X_word_tfidf.shape)\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "X_word_tfidf_train = X_word_tfidf[0:train.shape[0],]\n",
    "X_word_tfidf_test = X_word_tfidf[train.shape[0]:X_word_tfidf.shape[0],]\n",
    "\n",
    "print('-------Word TF-IDF Data Complete-------')\n",
    "\n",
    "print('\\n-------Ridge Model-------')\n",
    "s = time.time()\n",
    "ridge_model_word_tfidf = Ridge(alpha = 1.5)\n",
    "ridge_model_word_tfidf = ridge_model_word_tfidf.fit(X_word_tfidf_train, y_total)\n",
    "\n",
    "\n",
    "ridge_train_word_tfidf_res = ridge_model_word_tfidf.predict(X_word_tfidf_train)\n",
    "ridge_train_word_tfidf_res = [10 if i >10 else round(i) for i in ridge_train_word_tfidf_res]\n",
    "ridge_train_word_tfidf_res = np.array([0 if i<0 else round(i) for i in ridge_train_word_tfidf_res])\n",
    "\n",
    "print(\"      train accuracy:\", mean_squared_error(ridge_train_word_tfidf_res, y_total))\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "\n",
    "ridge_test_word_tfidf_res = ridge_model_word_tfidf.predict(X_word_tfidf_test)\n",
    "print('-------Word TF-IDF Ridge Model Complete-------')\n",
    "\n",
    "\n",
    "print('\\n-------Transform to Features(char level)-------')\n",
    "comment_char_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='char'))\n",
    "\n",
    "feature_union_char = make_union(\n",
    "    onehot_pipeline,\n",
    "    comment_char_pipeline\n",
    ")\n",
    "X_char = feature_union_char.fit_transform(total_data)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('      Add Length and Upper Prob to Features......')\n",
    "s = time.time()\n",
    "length = [(X_char[i,]!=0).sum() for i in range(X_char.shape[0])]\n",
    "new_l = scale(length)\n",
    "X_char = hstack([X_char,new_l],format='csr')\n",
    "X_char = hstack([X_char,new_up],format='csr')\n",
    "\n",
    "print('      X_char shape: ',X_char.shape)\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "\n",
    "X_char_train = X_char[0:train.shape[0],]\n",
    "X_char_test = X_char[train.shape[0]:X_char.shape[0],]\n",
    "\n",
    "print('-------Character Data Complete-------')\n",
    "\n",
    "print('\\n-------Ridge Model-------')\n",
    "s = time.time()\n",
    "ridge_char_model = Ridge(alpha = 1.5)\n",
    "ridge_char_model = ridge_char_model.fit(X_char_train, y_total)\n",
    "\n",
    "\n",
    "ridge_char_train_res = ridge_char_model.predict(X_char_train)\n",
    "ridge_char_train_res = [10 if i >10 else round(i) for i in ridge_char_train_res]\n",
    "ridge_char_train_res = np.array([0 if i<0 else round(i) for i in ridge_char_train_res])\n",
    "\n",
    "print(\"      train accuracy:\", mean_squared_error(ridge_char_train_res, y_total))\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "\n",
    "ridge_test_char_res = ridge_char_model.predict(X_char_test)\n",
    "print('-------Character Ridge Model Complete-------')\n",
    "\n",
    "\n",
    "print('\\n-------Transform to Features(count level)-------')\n",
    "comment_word_cv_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='cv', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union_word_cv = make_union(\n",
    "    onehot_pipeline,\n",
    "    comment_word_cv_pipeline\n",
    ")\n",
    "X_word_cv = feature_union_word_cv.fit_transform(total_data)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('      Add Length and Upper Prob to Features......')\n",
    "s = time.time()\n",
    "length = [(X_word_cv[i,]!=0).sum() for i in range(X_word_cv.shape[0])]\n",
    "new_l = scale(length)\n",
    "X_word_cv = hstack([X_word_cv,new_l],format='csr')\n",
    "X_word_cv = hstack([X_word_cv,new_up],format='csr')\n",
    "\n",
    "print('      X_word_cv shape: ',X_word_cv.shape)\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "X_word_cv_train = X_word_cv[0:train.shape[0],]\n",
    "X_word_cv_test = X_word_cv[train.shape[0]:X_word_cv.shape[0],]\n",
    "\n",
    "print('-------Word Count Vectorize Data Complete-------')\n",
    "\n",
    "print('\\n-------Ridge Model-------')\n",
    "s = time.time()\n",
    "ridge_word_cv_model = Ridge(alpha = 1.5)\n",
    "ridge_word_cv_model = ridge_word_cv_model.fit(X_word_cv_train, y_total)\n",
    "\n",
    "\n",
    "ridge_word_cv_train_res = ridge_word_cv_model.predict(X_word_cv_train)\n",
    "ridge_word_cv_train_res = [10 if i >10 else round(i) for i in ridge_word_cv_train_res]\n",
    "ridge_word_cv_train_res = np.array([0 if i<0 else round(i) for i in ridge_word_cv_train_res])\n",
    "\n",
    "print(\"      train accuracy:\", mean_squared_error(ridge_word_cv_train_res, y_total))\n",
    "print('      time elapsed: ', time.time()-s)\n",
    "\n",
    "ridge_word_cv_test_res = ridge_word_cv_model.predict(X_word_cv_test)\n",
    "print('-------Word Count Vectorize Ridge Model Complete-------')\n",
    "\n",
    "print('\\n-------Ensemble 3 Results-------')\n",
    "result = 0.15*ridge_test_char_res+0.8*ridge_test_word_tfidf_res+0.05*ridge_word_cv_test_res\n",
    "result = [10 if i >10 else round(i) for i in result]\n",
    "result = [0 if i<0 else round(i) for i in result]\n",
    "# print(\"test accuracy:\", mean_squared_error(result, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
